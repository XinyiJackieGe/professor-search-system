<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"
  xmlns:vcard="http://www.ais.columbia.edu/sws/xmlns/vcard3.0#"
  xmlns:dc="http://purl.org/dc/elements/1.0/"
  xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
  xmlns:cfs="http://www.ais.columbia.edu/sws/xmlns/cufs#"
  xmlns:cms="http://www.ais.columbia.edu/sws/xmlns/cucms#">
  <head>
    <title>Julia Hirschberg</title>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta content="text/html; charset=UTF-8" http-equiv="Content-Type">
    <meta content="Julia Hirschberg" name="author">
    <meta content="SEAS Columbia University;" name="Description">
    <meta content="SEAS Columbia University;" name="Keywords">
    <style media="all" type="text/css">@import "hirschberg_style.css";</style>
    <link
href="http://wwwapp.cc.columbia.edu/sws/cucms/civil/admin/images/favicon.ico"
      rel="shortcut icon">
    <link
href="http://wwwapp.cc.columbia.edu/sws/cucms/civil/admin/images/favicon.ico"
      rel="icon">
    <script type="text/javascript" src="scripts/dropdowntabs.js"></script>
  </head>
  <body>
    <!--Start Frame -->
    <div id="frame">
      <div id="banner"><!--Start Department Banner --> <a
          name="backtotop" id="backtotop"></a> <a
          href="http://www.cs.columbia.edu"> <img
            src="http://www.cs.columbia.edu/images/cscu.jpg"
            alt="Computer Science at Columbia University" height="81"
            width="100" border="0"></a><a
          href="http://www.cs.columbia.edu"><img
            src="http://www.cs.columbia.edu/images/cscutitle.jpg"
            alt="Computer Science at Columbia University"
            class="imgmargins"></a><img src="images/hirchberg_title.jpg"
          alt="Julia Hirschberg" class="title"></div>
      <!--End Department Banner -->
      <div id="contentFrame"><!--Start Content Frame -->
        <div id="mainnav"><!--Start Navigation -->
          <div id="bluemenu" class="bluetabs">
            <ul>
              <li class="current"><a href="index.html">Home</a></li>
              <li><a href="research_summary.htm" rel="dropmenu2_b">Research</a></li>
              <li><a href="publications_patents.htm" rel="dropmenu3_b">Publications</a></li>
              <li><a href="teaching.htm">Teaching</a></li>
              <li><a href="service_journal.htm" rel="dropmenu5_b">Service</a></li>
              <li><a href="awards.htm">Awards &amp; Honors</a></li>
              <li><a href="slpg.htm">Spoken Language Processing Group</a></li>
            </ul>
          </div>
          <!-- Start Content for Dropdown Menus -->
          <!--Research drop down menu -->
          <div id="dropmenu2_b" class="dropmenudiv_b" style="width:
            140px;"> <a href="research_summary.htm">Summary</a> <a
              href="research_projects.htm">Past and Present Projects</a>
            <a href="research_talks.htm">Recent Talks</a> </div>
          <!--Publications drop down menu -->
          <div id="dropmenu3_b" class="dropmenudiv_b" style="width:
            140px;"> <a href="publications_patents.htm">Patents</a> <a
              href="publications_grants.htm">Grants</a> <a
              href="publications_refereed.htm">Refereed Publications</a>
            <a href="publications_books.htm">Book Chapters &amp; Edited
              Volumes</a> </div>
          <!--Service drop down menu -->
          <div id="dropmenu5_b" class="dropmenudiv_b" style="width:
            140px;"> <a href="service_journal.htm">Journal
              Responsibilities</a> <a href="service_professional.htm">Professional

              Activities</a> <a href="service_talks.htm">Invited Talks,
              Panels, Workshops, and Tutorials</a> <a
              href="service_conference.htm">Conference Presentations</a>
            <a href="service_columbia.htm">Columbia Service</a> </div>
          <script type="text/javascript">
					//SYNTAX: tabdropdown.init("menu_id", [integer OR "auto"])
					tabdropdown.init("bluemenu")
					</script> </div>
        <!-- End Navigation -->
        <div id="content"><!--Start Content --> <img
            src="images/title_research.jpg" alt="Research" class="title">
          <div id="leftColumn"><!--Start Left Column -->
            <p><a href="research_summary.htm">Summary</a> | <a
                href="research_projects.htm" class="current">Past and
                Present Projects</a> | <a href="research_talks.htm">Recent

                Talks</a><br>
            </p>
            <h2>More recent projects can be found on the <a href="http://www.cs.columbia.edu/speech/projects.cgi"> Speech Lab page</a><br>
            </h2>
            <h2 style="background:white"> <st1:place w:st="on"> <st1:placename
                  w:st="on"><span style="font-size:11.0pt">Past</span></st1:placename>
                <span style="font-size:11.0pt"> <st1:placetype
                    w:st="on">Projects</st1:placetype> </span></st1:place>
              <span style="font-size:11.0pt"> <o:p></o:p> </span></h2>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                11.0pt;font-family:Verdana">Emotional Speech <o:p></o:p>
              </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Jennifer
                Venditti, Jackson Liscombe, and I have looked at methods
                of eliciting both subjective and objective judgments and
                of correlating judgments of single tokens on multiple
                emotion scale -- i.e., if subjects rate a token high for
                <em><span style="font-family:Verdana">frustration, </span></em>what

                other emotional states do they also rate it high for --
                or low ("<a href="files/euro03-emotion.pdf"
                  target="_blank">Classifying Subjective Ratings of
                  Emotional Speech</a>," Eurospeech 2003). We conducted
                eye-tracking experiments which allow us to compare
                subjective judgments to more objective cues to the
                decision process. We have also worked with colleagues at
                the University of Pittsburgh to study speaker state
                student speech in a tutorial system for emotional states
                such as <em><span style="font-family:Verdana">anger,
                    frustration, confidence</span></em> and <em><span
                    style="font-family:Verdana">uncertainty</span></em>
                ("<a href="files/liscombe_al_05b.pdf" target="_blank">Detecting


                  Certainness in Spoken Tutorial Dialogues</a>,"
                INTERSPEECH 2005). We have also studied question form
                and function in this domain and performed machine
                learning experiments to identify Question-Bearing Turns,
                as well as question form and function, automatically </span><span
                style="font-size:8.5pt;
                mso-bidi-font-size:12.0pt;font-family:Verdana">(“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2006/liscombe_al_06.pdf">Detecting


                  question-bearing turns in spoken tutorial dialogues</a>”
                and </span><span
                style="font-size:8.5pt;mso-bidi-font-size:11.5pt;font-family:Verdana;
                color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2006/venditti_al_06.pdf">Intonational


                  cues to student questions in tutoring dialogs</a>”,
                INTERSPEECH 2006).<span style="mso-spacerun:yes">&nbsp;
                </span>Agus Gravano, Elisa Sneed, Gregory Ward and I
                have also looked at intonational contour and syntactic
                construction in the conveyance of speaker certainty (</span><span
                style="font-size:11.5pt;
                font-family:Garamond;color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2008/gravano_al_08.pdf">The

                  effect of contour type and epistemic modality on the
                  assessment of speaker certainty</a>”, Speech Prosody
                2008</span><span
                style="font-size:8.5pt;mso-bidi-font-size:11.5pt;
                font-family:Verdana;color:black">), and Frank Enos and I
                have proposed a new methodology for eliciting emotional
                speech in </span><span style="font-size:
                11.5pt;font-family:Garamond;color:black">“<a
href="http://www1.cs.columbia.edu/nlp/papers/2006/enos_hirschberg_06.pdf">A
                  framework for eliciting emotional speech: Capitalizing
                  on the actor's process</a>”, LREC Workshop on
                Emotional Corpora.</span><span style="font-size:8.5pt;
                mso-bidi-font-size:12.0pt;font-family:Verdana"> <o:p></o:p>
              </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                11.0pt;font-family:Verdana">Deceptive Speech</span><span
                style="font-size:9.5pt;
                mso-bidi-font-size:8.5pt;font-family:Verdana"> <o:p></o:p>
              </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">&nbsp;Frank
                Enos, Stefan Benus, and I are working with colleagues at
                SRI/ICSI and the University of Colorado on automatic
                methods of distinguishing deceptive from non-deceptive
                speech ("<a href="files/hirschberg_al_05.pdf"
                  target="_blank">Distinguishing Deceptive from
                  Non-Deceptive Speech</a>," INTERSPEECH 2005; </span><span
style="font-size:11.5pt;font-family:Garamond;color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2007/enos_al_07.pdf">Detecting


                  deception using critical segments</a>”, </span><span
                style="font-size:8.5pt;
mso-bidi-font-size:11.5pt;font-family:Verdana;color:black">INTERSPEECH
                2007</span><span
                style="font-size:8.5pt;font-family:Verdana">).&nbsp;<span
                  style="mso-spacerun:yes">&nbsp; </span>For this work
                we collected and annotated a large corpus of deceptive
                and non-deceptive speech, the CSC Deception Corpus. We
                have also looked at the role of pausing in deception (</span><span
                style="font-size:8.5pt;mso-bidi-font-size:11.5pt;font-family:Verdana;

                color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2006/benus_al_06.pdf">Pauses

                  in deceptive speech</a>”</span><span
                style="font-size:8.5pt;font-family:Verdana">; Speech
                Prosody 2006) and examined the role of personality in
                the accuracy of human judges of deception (</span><span
                style="font-size:8.5pt;mso-bidi-font-size:
                11.5pt;font-family:Verdana;color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2006/enos_al_06.pdf">Personality


                  factors in human deception detection: Comparing human
                  to machine performance</a>”, INTERSPEECH 2006</span><span
                style="font-size:8.5pt;font-family:Verdana">). <o:p></o:p>
              </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                11.0pt;font-family:Verdana;color:windowtext">Charismatic
                Speech <o:p></o:p> </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Andrew
                Rosenberg, Fadi Biadsy, and I are study the acoustic,
                prosodic, and lexical cues to <em><span
                    style="font-family:Verdana">charismatic speech </span></em>in


                American English ("<a
                  href="files/rosenberg_hirschberg_05.pdf"
                  target="_blank">Acoustic/Prosodic and Lexical
                  Correlates of Charismatic Speech</a>",
                &nbsp;INTERSPEECH 2005).&nbsp; With Fadi Biadsy &nbsp;we
                have extended our effort to include research on
                Palestinian Arabic, and with Rolf Carlson (KTH) and Eva
                Stangert (Umeå) we have investigated cross-cultural
                perceptions of charisma and their acoustic, prosodic and
                lexical features (</span><span
                style="font-size:11.5pt;font-family:Garamond;color:black">“<a
href="http://www1.cs.columbia.edu/nlp/papers/2008/fadi_al_08b.pdf">A
                  cross-cultural comparison of American, Palestinian,
                  and Swedish perception of charismatic speech</a>”,
                Speech Prosody 2008)</span><span style="font-size:
                8.5pt;font-family:Verdana">. <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Speech


                Summarization and Distillation <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;mso-bidi-font-size:
                12.0pt;font-family:Verdana">With Sameer Maskey, Andrew
                Rosenberg, and Fadi Biadsy, I have worked on speech
                summarization, exploring new techniques which take
                advantage of prosodic and acoustic information, in
                addition to lexical cues and structural cues, in news
                broadcasts to 'gist' a broadcast (</span><span
                style="font-size:11.5pt;font-family:Garamond;color:black">“<a
href="http://www1.cs.columbia.edu/nlp/papers/2003/maskey_hirschberg_03.pdf">Automatic


                  speech summarization of broadcast news using
                  structural features</a>”, EUROSPEECH 2003;</span><span
                style="font-size:8.5pt;mso-bidi-font-size:12.0pt;
                font-family:Verdana"> "<a
                  href="files/eurospeech05_vfinal.pdf" target="_blank"><span
                    style="mso-bidi-font-size:8.5pt">Comparing Lexical,
                    Acoustic/Prosodic, Structural and Discourse Features
                    for Speech Summarization</span></a>," INTERSPEECH
                2005; "<a href="files/hlt_06_summ.pdf" target="_blank"><span
                    style="mso-bidi-font-size:8.5pt">Summarizing Speech
                    without Text Using Hidden Markov Models</span></a>,"
                HLT/NAACL 2006; and “Intonational Phrases for Speech
                Summarization”, INTERSPEECH 2008).&nbsp; We have also
                looked at the segmentation of news broadcasts into
                stories ("<a
href="http://www1.cs.columbia.edu/nlp/papers/2006/rosenberg_hirschberg_06a.pdf"
                  target="_blank"><span style="mso-bidi-font-size:8.5pt">Story

                    Segmentation of Broadcast News in English, Mandarin
                    and Arabic</span></a>" HLT/NAACL 2006), the
                determination of speaker roles (e.g. anchor, reporter,
                interviewee ) (See R. Barzilay et al., "<a
                  href="files/aaai00.ps" target="_blank"><span
                    style="mso-bidi-font-size:8.5pt">Identification of
                    Speaker Role in Radio Broadcasts</span></a>", AAAI
                2000 for earlier work.), and the extraction of </span><em><b><span
                    style="font-size:8.5pt;font-family:Verdana">soundbites

                  </span></b></em><span
                style="font-size:8.5pt;mso-bidi-font-size:12.0pt;font-family:Verdana">from


                broadcasts (spoken ‘quotes’ included in a show) and
                identification of their speaker, .&nbsp; </span><span
                style="font-size:8.5pt;mso-bidi-font-size:11.5pt;
                font-family:Verdana;color:black"><a
                  href="http://www1.cs.columbia.edu/nlp/papers/2008/fadi_al_08a.pdf">“An


                  unsupervised approach to biography production using
                  Wikipedia”,</a> ACL/NAACL 2008.<span
                  style="mso-spacerun:yes">&nbsp;&nbsp; </span>Elena
                Filatova, Martin Jansche, </span><span
                style="font-size:11.5pt;font-family:Garamond;color:black">Mehrbod

                Sharifi</span><span
                style="font-size:8.5pt;mso-bidi-font-size:11.5pt;font-family:Verdana;
                color:black">, and Wisam Dakka are co-authors of some of
                this work also.<br
                  style="mso-special-character:line-break">
                <!--[if !supportLineBreakNewLine]--> <br
                  style="mso-special-character:line-break">
                <!--[endif]--> </span><span
                style="font-size:8.5pt;mso-bidi-font-size:12.0pt;
                font-family:Verdana"> <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Spoken

                Dialogue Systems <o:p></o:p> </span></h2>
            <h3 style="background:white"><span
                style="font-size:9.5pt;font-family:Verdana">The <st1:place
                  w:st="on"> <st1:city w:st="on">Columbia</st1:city> </st1:place>
                Games Corpus <o:p></o:p> </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Agus
                Gravano, Stefan Benus, and I have been collecting and
                analyzing a large corpus of spontaneous dialogues,
                produced by subjects playing a computer game we
                created.&nbsp; We collected this data to test several
                theories of the way speakers produce ‘given’ (as opposed
                to ‘new’) information.&nbsp; We are currently labeling
                this corpus for intonation, in the ToBI framework; we
                have also turn-taking behaviors, cue phrases, questions
                (identified as to form and function) and other aspects
                of the corpus.&nbsp; This is joint work with Gregory
                Ward and Elisa Sneed at <st1:place w:st="on"> <st1:placename
                    w:st="on">Northwestern</st1:placename> <st1:placetype
                    w:st="on">University</st1:placetype> </st1:place> .<span
                  style="mso-spacerun:yes">&nbsp;&nbsp; </span>Michael
                Mulley also was an active participant in the design of
                the corpus. <o:p></o:p> </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                11.0pt;font-family:Verdana;color:windowtext">Cue Phrases
                <o:p></o:p> </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Work on cue
                phrases, or discourse markers, is described in Julia
                Hirschberg and Diane Litman, "<a href="files/cl92.ps"
                  target="_blank">Empirical Studies on the
                  Disambiguation of Cue Phrases</a>," Computational
                Linguistics, 1992; some figures are missing in this
                version. &nbsp;More recently Agus Gravano, Stefan Benus,
                Lauren Wilcox, Hector Chavez, and Shira Mitchell, Ilia
                Vovsha, and I have been looking at cue phrase production
                and detection in the Games corpus (</span><span
                style="font-size:11.5pt;font-family:Garamond;
                color:black">“<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2007/gravano_al_07a.pdf">On

                  the role of context and prosody in the interpretation
                  of <i>okay</i></a>”, ACL 2007; “<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2007/gravano_al_07b.pdf">Classification


                  of discourse functions of affirmative words in spoken
                  dialogue</a>”, Interspeech 2007; “<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2007/benus_al_07b.pdf">The

                  prosody of backchannels in American English</a>”,
                ICPhS 2007</span><span
                style="font-size:8.5pt;font-family:Verdana">).<span
                  style="mso-spacerun:yes">&nbsp; </span> <o:p></o:p>
              </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                8.5pt;font-family:Verdana">Speaker Entrainment <o:p></o:p>
              </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Ani Nenkova,
                Agus Gravano and I are looking at various types of
                speaker entrainment in the Games Corpus (</span><span
                style="font-size:11.5pt;font-family:Garamond;
                color:black">“High <a
                  href="http://www1.cs.columbia.edu/nlp/papers/2008/nenkova_al_08.pdf">frequency


                  word entrainment in spoken dialogue</a>”, ACL 2008</span><span
                style="font-size:8.5pt;font-family:Verdana">).<span
                  style="mso-spacerun:yes">&nbsp; </span>We are also
                examining acoustic/prosodic entrainment. <o:p></o:p> </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                8.5pt;font-family:Verdana">The Given/New Distinction <o:p></o:p>
              </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;mso-bidi-font-size:
                11.5pt;font-family:Verdana;color:black">Agus Gravano,
                Ani Nenkova, Gregory Ward, Elisa Sneed and I have
                studied the different ways speakers produce ‘given’ vs.
                ‘new’ information in “<a
href="http://www1.cs.columbia.edu/nlp/papers/2006/gravano_hirschberg_06.pdf">Effect


                  of genre, speaker, and word class on the realization
                  of given and new information</a>”, INTERSPEECH 2006
                and “<a
                  href="http://www1.cs.columbia.edu/nlp/papers/2007/hirschberg_al_07.pdf">Intonational


                  overload: Uses of the H* !H* L- L% contour in read and
                  spontaneous speech”</a>, Laboratory Phonology 9.</span><span
                style="font-size:8.5pt;font-family:Verdana"> <o:p></o:p>
              </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;font-family:Verdana">Misrecognitions,


                Corrections, and Error Awareness <o:p></o:p> </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana"><a
                  href="http://www.cs.pitt.edu/%7Elitman">Diane Litman</a>,
                Marc Swerts and I have studied the prosodic consequences
                of recognition errors in Spoken Dialogue Systems. We are
                studying whether prosodic features of user utterances
                can tell us a) whether a speech recognition error has
                occurred, as a user reacts to it (e.g. System: "Did you
                say you want to go to <st1:city w:st="on"> <st1:place
                    w:st="on">Baltimore</st1:place> </st1:city> ?"
                User: "NO!"), or, b) whether a user is in fact
                correcting such a recognition error (e.g. User: "I want
                to go to <st1:place w:st="on"> <st1:city w:st="on">BOSTON</st1:city>
                </st1:place> !". We have already found that prosodic
                features predict recognition errors directly with
                considerable accuracy in the <a
                  href="http://www.research.att.com/%7Ediane/TOOT.html">TOOT</a>
                train information corpus dialogues. Using machine
                learning techniques, we have found that, in combination
                with information already available to the recognizer,
                such as acoustic confidence scores, grammar, and
                recognized string, prosodic information can distinguish
                speaker turns that are misrecognized far better than
                traditional methods for ASR rejection using acoustic
                confidence scores alone. See Julia Hirschberg, Diane
                Litman and Marc Swerts, "<a href="files/specom04.pdf"
                  target="_blank">Prosodic and Other Cues to Speech
                  Recognition Failures</a>," Speech Communication 2004.
                We have also studied user corrections of system errors
                in the TOOT corpus, finding also significant prosodic
                differences between corrections and non-corrections that
                can be used to predict when a user is correcting the
                system with some success; in addition we find
                interesting and useful correlations between system
                strategies and types of user corrections, as well as
                evidence for what types of corrections are more
                successful (see "<a href="files/icslp00-corr.ps"
                  target="_blank">Corrections in Spoken Dialogue Systems</a>,"

                ICSLP-00; "<a href="files/naacl01.pdf" target="_blank">Identifying

                  User Corrections Automatically in Spoken Dialogue
                  Systems</a>", NAACL-01; and “<a
                  href="http://www.aclweb.org/anthology-new/J/J06/J06-3004.pdf">Characterizing


                  and Prediction Corrections in Spoken Dialogue</a>”,
                Computational Linguistics 32, 2006). <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Predicting

                Prosodic Events <o:p></o:p> </span></h2>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                11.0pt;font-family:Verdana;color:windowtext">Intonational

                Variation in Synthetic Speech <o:p></o:p> </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Most of my
                early work on predicting intonational phrase boundaries
                and prominences was done in the Text-to-Speech synthesis
                group at Bell Labs.<span style="mso-spacerun:yes">&nbsp;
                </span>Some papers describing that work are Philipp
                Koehn, Steven Abney, Julia Hirschberg, and Michael
                Collins, "<a href="files/koehn99.ps" target="_blank">Improving

                  Intonational Phrasing with Syntactic Information</a>,"
                ICASSP-00; Julia Hirschberg and Pilar Prieto, "<a
                  href="files/specom96.ps" target="_blank">Training
                  intonational phrasing rules automatically for English
                  and Spanish Text-to-Speech</a>," Speech Communication,
                1996; Julia Hirschberg, "<a href="files/aij93.ps"
                  target="_blank">Pitch Accent in Context: Predicting
                  Intonational Prominence from Text</a>," Artificial
                Intelligence, 1993; and Michelle Wang and Julia
                Hirschberg, "<a href="files/csl92.ps" target="_blank">Automatic


                  Classification of Intonational Phrase Boundaries</a>,"
                Computer Speech and Language, 1992.<span
                  style="mso-spacerun:yes">&nbsp;&nbsp; </span>These
                methods were used to assign intonational variation
                automatically in the Bell Labs Text-to-Speech System.<span
                  style="mso-spacerun:yes">&nbsp; </span>I also
                collaborated &nbsp;on two projects in concept-to-speech
                generation (generating speech from an abstract
                representation of the concepts to be conveyed). One,
                with Shimei Pan and <a
                  href="http://www.cs.columbia.edu/%7Ekathy">Kathy
                  McKeown</a> of <st1:place w:st="on"> <st1:placename
                    w:st="on">Columbia</st1:placename> <st1:placetype
                    w:st="on">University</st1:placetype> </st1:place> ,
                seeks to assign prosody appropriately for a multimodal
                medical application, <a
                  href="http://www.cs.columbia.edu/nlp/projects.html">MAGIC</a>.
                Some results are documented in Shimei Pan, Kathy McKeown
                and Julia Hirschberg, "<a href="files/euro01-pan.ps"
                  target="_blank">Semantic Abnormality and its
                  Realization in Spoken Language</a>," Proceedings of
                Eurospeech 2001, Aalborg. The other, with <a
                  href="http://www.research.att.com/%7Esrini">Srinivas
                  Bangalore</a>, Owen Rambow, and Marilyn Walker
                (AT&amp;T Labs -- Research) involves prosodic assignment
                in the DARPA Communicator travel information domain.
                Some results appear in Julia Hirschberg and Owen Rambow,
                "<a href="files/euro01-phr.ps" target="_blank">Learning
                  Prosodic Features using a Tree Representation</a>,"
                Proceedings of Eurospeech 2001, Aalborg. <o:p></o:p> </span></p>
            <h3 style="background:white"><span
                style="font-size:9.5pt;mso-bidi-font-size:
                8.5pt;font-family:Verdana">Detecting Prosodic Events <o:p></o:p>
              </span></h3>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">More recent
                work on prosody detection has been done with Andrew
                Rosenberg, who has developed new ways to combine
                energy-based features with other acoustic and lexical
                features to achieve very high accuracy in prediction.<span
                  style="mso-spacerun:yes">&nbsp; </span>Papers
                documenting this work include (</span><span
                style="font-size:11.5pt;font-family:Garamond;color:black">“<a
href="http://www1.cs.columbia.edu/nlp/papers/2006/rosenberg_hirschberg_06b.pdf">On


                  the correlation between energy and pitch accent in
                  read English speech</a>”, INTERSPEECH 2006; and “<a
href="http://www1.cs.columbia.edu/nlp/papers/2007/rosenberg_hirschberg_07a.pdf">Detecting


                  pitch accent using pitch-corrected energy-based
                  predictors”</a>, INTERSPEECH 2007)</span><span
                style="font-size:8.5pt;font-family:Verdana"> <o:p></o:p>
              </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Audio

                Browsing and Retrieval <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Work on our
                <a href="http://www.research.att.com/%7Epereira/SCAN">SCAN

                </a>(Spoken Content-Based Audio Navigation) browsing and
                retrieval system is summarized in John Choi et al., "<a
                  href="files/hlt_06_summ.pdf" target="_blank">Spoken
                  Content-Based Audio Navigation (SCAN)</a>," ICPhS-99.
                This project combines ASR and IR technology to enable
                search of large audio databases, such as broadcast news
                archives or voicemail. It started life as `AudioGrep'.
                Current collaborators include Steve Abney, Brian Amento,
                Michiel Bacchiani, Phil Isenhour, Diane Litman, Larry
                Stead, and Steve Whittaker. My particular interests lie
                in the use of acoustic information to segment audio
                (Julia Hirschberg and Christine Nakatani, "<a
                  href="files/icslp98.ps" target="_parent">Acoustic
                  Indicators of Topic Segmentation</a>," ICSLP-98) and
                the study of how people browse and search audio
                databases such as broadcast news collections (Steve
                Whittaker et al., "<a href="files/sigir99.ps"
                  target="_blank">SCAN: Designing and Evaluating User
                  Interfaces to Support Retrieval from Speech Archives </a>",

                SIGIR-99) and voicemail (Steve Whittaker, Julia
                Hirschberg and Christine Nakatani, "<a
                  href="files/chi98-browser.ps" target="_blank">Play it
                  again: a study of the factors underlying speech
                  browsing behavior</a>," and Steve Whittaker, Julia
                Hirschberg and Christine Nakatani, "<a
                  href="files/chi98-vmail.ps" target="_blank">All talk
                  and all action: strategies for managing voicemail
                  messages</a>," both presented at CHI-98). We have also
                studied how differences in ASR accuracy (comparing 100%,
                84%, 69%, 50% accuracy transcripts) affect users'
                ability to perform tasks, finding effects for transcript
                accuracy on time to solution, amount of speech played,
                likelihood of subjects abandoning transcript, and
                various subjective measures; however, our results hold
                only when we collapse our four categories into two;
                i.e., there are no differences between perfect and 84%
                accurate transcripts or between 69% and 50% accurate
                ones (Litza Stark, Steve Whittaker, and Julia
                Hirschberg, "<a href="files/icslp00-asr.ps"
                  target="_blank">ASR Satisficing: The effects of ASR
                  accuracy on speech retrieval</a>", ICSLP-00).
                Currently, in a new voicemail application, SCANMail, now
                in friendly trial, we have ported SCAN technology to the
                voicemail domain: users are able to browse and retrieve
                their voicemail by content. See J. Hirschberg et al., "<a
                  href="files/euro01-sm.ps" target="_blank">SCANMail:
                  Browsing and Searching Speech Data by Content Domain</a>"
                and A. Rosenberg et al., "<a href="files/euro01-aer.ps"
                  target="_blank">Caller Identification for the SCANMail
                  Voicemail Browser</a>" (both presented at Eurospeech
                2001). Meredith Ringel and I have also worked on ranking
                voicemail messages as to urgency and distinguishing
                personal from business methods, using machine learning
                techniques ("<a href="files/msgPriority.pdf"
                  target="_blank">Automated Message Prioritization:
                  Making Voicemail Retrieval More Efficient</a>,"
                presented at CHI 2002). <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Intonation

                and Discourse Structure <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Some results
                of a long collaboration with Barbara Grosz and Christine
                Nakatani on the intonational correlates of discourse
                structure in read and spontaneous speech is described in
                "<a href="files/acl96.ps" target="_blank">A Prosodic
                  Analysis of Discourse Segments in Direction-Giving
                  Monologues</a>," (ACL-96). The BDC corpus (with ToBI
                labels) is available <a
                  href="http://www.cs.columbia.edu/%7Ejulia/bdc.tar.Z">here</a>.&nbsp;

                Results of earlier studies of read speech are described
                in "<a href="files/banffreg.ps" target="_blank">Some
                  Intonational Characteristics of Discourse Structure</a>,"

                (a reformatted version of ICSLP-92). <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Intonational


                Disambiguation <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Empirical
                studies comparing the way native speakers of different
                languages employ intonational variation to disambiguate
                potentially ambiguous utterances are described in Julia
                Hirschberg and Cinzia Avesani, "<a
                  href="files/esca97.ps" target="_blank">The Role of
                  Prosody in Disambiguating Potentially Ambiguous
                  Utterances in English and Italian</a>," ESCA Tutorial
                and Research Workshop on Intonation, Athens, 1997. <o:p></o:p>
              </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Disfluencies

                in Spontaneous Speech <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">Christine
                Nakatani and Julia Hirschberg, "<a
                  href="files/jasa93.pdf" target="_blank">A Corpus-based
                  study of repair cues in spontaneous speech</a>," JASA,
                1994, describes studies of the acoustic/prosodic
                characteristics of self-repairs. <o:p></o:p> </span></p>
            <h2 style="background:white"><span style="font-size:11.0pt">Labeling

                Conventions and Labeled Corpora <o:p></o:p> </span></h2>
            <h2 style="background:white"><span style="font-size:11.0pt">
                <o:p>&nbsp;</o:p> </span></h2>
            <p style="background:white"><span
                style="font-size:8.5pt;font-family:Verdana">I have been
                an active participant in the development of the <a
                  href="http://www.ling.ohio-state.edu/%7Etobi/">ToBI
                  Labeling Standard </a>for the prosodic labeling of
                Standard American English (see the <a
                  href="files/conv.pdf" target="_blank">ToBI conventions</a>
                for a quick overview). . This standard was developed by
                a number of researchers from industry and academia and
                has been extended for other dialects of English and for
                other languages, including Italian, German, Spanish,
                Japanese and more. Interlabeler reliability ratings (see
                John Pitrelli, Mary Beckman, and Julia Hirschberg, "<a
href="http://www.ling.ohio-state.edu/%7Etobi/ame_tobi/Pitrelli_etal1994.pdf">Evaluation


                  of Prosodic Transcription Labeling Reliability in the
                  ToBI Framework,</a>" Proceedings of the Third
                International Conference on Spoken Language Processing,
                Yokohama, September, 1994, pp. 123-126) are quite good
                and there are tools and <a
                  href="http://www.ling.ohio-state.edu/%7Etobi/ame_tobi/">training


                  materials available</a> with pdf and html versions and
                praat files there. There is also a <a
                  href="http://www.speech.kth.se/wavesurfer/download.html">Wavesurfer</a>
                version and another <a
                  href="http://www.fon.hum.uva.nl/praat/">Praat</a>
                version with cardinal examples done by <a
                  href="http://www.cs.columbia.edu/%7Eagus">Agus Gravano</a>
                and available from the <a
                  href="http://www.cs.columbia.edu/%7Eagus/tobi">Columbia

                  ToBI site</a>.&nbsp; The Boston Directions Corpus
                (with ToBI labels) is available <a
                  href="http://www.cs.columbia.edu/%7Ejulia/bdc.tar.Z">here</a>.&nbsp;&nbsp;&nbsp;


                <o:p></o:p> </span></p>
            <h2>&nbsp;</h2>
            <p>&nbsp;</p>
          </div>
          <div id="rightColumn"><!-- Start Right Column -->
            <p><img src="images/hirschberg_portrait.jpg" alt="Julia
                Hirshberg Portrait" height="150" width="120"> </p>
            <p>Julia Hirschberg<br>
              Percy K. and Vida L. W. Hudson Professor of Computer Science</p>
            <p>Columbia University<br>
              Department of Computer Science<br>
              1214 Amsterdam Avenue<br>
              M/C 0401<br>
              450 CS Building<br>
              New York, NY 10027</p>
            <p>email: <a href="mailto:julia@cs.columbia.edu">julia@cs.columbia.edu</a><br>
              phone: (212) 939-7114</p>
            <p><a href="files/cv.pdf" target="_blank">Download CV</a></p>
          </div>
          <!-- End Right Column --> </div>
        <!-- End Content -->
        <div id="address"><!--Start Address --> <span class="boldText">
            <a href="http://www.columbia.edu">Columbia University</a>
            Department of Computer Science </span> / <a
            href="http://www.engineering.columbia.edu">Fu Foundation
            School of Engineering &amp; Applied Science</a><br>
          450 Computer Science Building / 1214 Amsterdam Avenue,
          Mailcode: 0401 / New York, New York 10027-7003<br>
          Tel: 1.212.939.7000 / Fax: 1.212.666.0140<br>
          <br>
        </div>
        <!--End Address --> </div>
      <!--End Frame --> </div>
  </body>
</html>
